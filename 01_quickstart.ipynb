{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c26d84-d3a8-445f-9250-cd1c6eef2c58",
   "metadata": {},
   "source": [
    "# Tutorial 01: quickstart\n",
    "\n",
    "Let's train a quick set of black-and-white patches to disguise a toy car from an YOLO model pretrained on MSCOCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae98af7-ab14-45f6-9ad1-3584d28509d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6463b-d9e9-4bdf-b393-7e0c49a0b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import electricmayhem.whitebox as em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b515f2d-27b6-4fe2-8b49-e6e88c6055c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_CLASSES = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "                'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench','bird', 'cat',\n",
    "                'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
    "                'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "                 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "                'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "                 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
    "                'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "                'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
    "                'clock', 'vase', 'scissors', 'teddy bear', 'hair drier','toothbrush']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc4dc5-9c1a-4483-a24e-552d942c4dd7",
   "metadata": {},
   "source": [
    "## create\n",
    "\n",
    "We're going to train single-channel patches, but are embedding in RGB images. We'll start the pipeline with `em.PatchStacker()` which will stack the patch into 3 channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f122f04-2541-459c-8643-f6712d6bb72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacker = em.PatchStacker(num_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850c70d-2aeb-49c2-ba01-25a6a10cd4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91c3e04f-d946-4efa-9175-e34ca48b8760",
   "metadata": {},
   "source": [
    "## implant\n",
    "\n",
    "We'll need to start with a dataset of target images in a pandas `DataFrame`. Each row will contain information on where to implant one patch in a target image (so if we're training multiple patches, one target image may take up several rows in the `DataFrame`. The dataset will have columns:\n",
    "\n",
    "* **image:** a path to the target image. Assumes all target images have the same dimensions.\n",
    "* **ulx**, **uly**, **llx**, **lly**, **urx**, **ury**, **lrx**, **lry:** pixel coordinates giving the corners of the patch within the image (\"upper left x\", etc)\n",
    "* **patch:** name of the patch (omit this if you're only training one)\n",
    "* **split:** whether this image is \"train\" or \"eval\". If you omit this column all target images will be used for train **and** eval. I'd strongly recommend against that though!\n",
    "\n",
    "This particular dataset contains 4 patches that were localized using aruco tags and painted out using Stable Diffusion. For this tutorial we'll only use the 3 patches on the car and skip the ground patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12147874-2a06-4260-99b1-f0dd118a2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"data/toycar/toycar_warp_dataset.csv\")\n",
    "labels = labels[labels.patch != \"ground\"]\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b46bbe-bba5-4d59-9989-e0950fb1f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980c810-5547-4851-adac-1f29bfce8330",
   "metadata": {},
   "source": [
    "Names of the 3 patches we'll train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f0eca-bb1f-4aa9-8523-0a5ddb92db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.patch.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60f27e-c516-4c7a-9885-7f079a59988d",
   "metadata": {},
   "source": [
    "The `em.WarpPatchImplanter()` class will take care of differentiably deforming and implanting patches (with kornia doing most of the heavy lifting). We need two inputs:\n",
    "\n",
    "* the `DataFrame` of target labels\n",
    "* a dictionary of patch shapes (at the point of implanting, so they'll be 3-channel); the implanter will use this to precompute transformation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567738a3-b66a-4eb6-a51a-0c748c0b8728",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_shapes = {k:(3,64,64) for k in ['hood', 'roof', 'door']}\n",
    "imp = em.WarpPatchImplanter(labels, patch_shapes=patch_shapes, dataset_name=\"toycar_warp_no_ground\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f851ec9-0e4b-401b-87d2-a12917e88e91",
   "metadata": {},
   "source": [
    "Quick visual check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88354da9-15a9-493b-9fc0-e2851d08097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.plot_boxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110320de-c592-4b0e-a4a4-c7d580f977f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8499553-ae01-4f9e-bc4e-14e707371c14",
   "metadata": {},
   "source": [
    "## compose\n",
    "\n",
    "The main tool `electricmayhem` has so far is `em.KorniaAugmentationPipeline()`, which just wraps the `kornia.augmentation` API. Initialize it with a dictionary of image augmentations, where each value is the keyword arguments that augmentation takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f468ee-8800-43e8-aaa6-c74d656a704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = em.KorniaAugmentationPipeline({\"ColorJiggle\":{\"brightness\":0.2, \"contrast\":0.2, \"hue\":0.1, \"saturation\":0.1},\n",
    "                                    \"RandomAffine\":{\"scale\":(0.9,1.1), \"shear\":10, \"padding_mode\":\"reflection\", \"degrees\":0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfaa816-0c15-4e3f-849e-2f15768a3cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfc18619-6d63-4590-ab6f-f44f68330739",
   "metadata": {},
   "source": [
    "## a quick visual check\n",
    "\n",
    "Let's put together the start of our pipeline, just using the create/implant/compose steps, and run some patches through it.\n",
    "\n",
    "First create the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d263b-bf24-46f8-92d8-3ee6bc0bf64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_start = stacker+imp+aug\n",
    "type(pipeline_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa12139-41d6-4f31-b6fc-3d58a3a74858",
   "metadata": {},
   "source": [
    "Then initialize a dictionary of **batches** of patches (in this case batchsize=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8088da4-188d-44b1-8308-59d548629cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = {k:0.5*torch.ones((1,1,64,64)).type(torch.float32) for k in ['hood', 'roof', 'door']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79064934-cbac-4f30-a2e1-76b45647ffaa",
   "metadata": {},
   "source": [
    "Pass the inputs through the pipeline. It will a return a tuple containing the output (a batch of implanted images) and a dictionary of additional information (not much there right now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed79fe3c-70b8-4f0b-8cbd-01a07e7fede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, kwargs = pipeline_start(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fffab-7b9f-4a8c-b507-bccef3556dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c28030-ba1c-478b-af6d-36f5c6197f60",
   "metadata": {},
   "source": [
    "If we run the patches through in `evaluate` mode the results should look similar- but will be pulled from a different set of target images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e9dca-189c-449a-a396-aa5cba68001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_eval, kwargs = pipeline_start(patches, evaluate=True)\n",
    "em.plot(output_eval[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b55431-169c-4944-bd59-ddda647d2ff1",
   "metadata": {},
   "source": [
    "And if we run the patches through in `control` mode it should repeat the last batch **exactly**, but without the patch implanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89eb93-6d02-41da-954c-209ba446d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_control, kwargs = pipeline_start(patches, evaluate=True, control=True)\n",
    "em.plot(output_control[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1dc60-6167-4969-b55e-9e8fbfab09c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be722a5c-0d4e-44f7-92b0-6048ba741a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65122add-2036-4cb1-ae09-ec2221c491b5",
   "metadata": {},
   "source": [
    "## infer\n",
    "\n",
    "When you load a model using the `ultralytics` library, the `ultralytics.models.yolo.model.YOLO` object it returns isn't really designed for doing adversarial attacks; we want to pull one of the lower-level objects out. \n",
    "\n",
    "* The `YOLO` object contains a `model` attribute that we can compute gradients through\n",
    "* Make sure you set it to `eval()` mode before training, to freeze the batchnorm layers and make sure outputs are in the appropriate format\n",
    "\n",
    "The `em.YOLOWrapper()` class reformats outputs from different YOLO versions into the v5 format; a list where the first element is:\n",
    "\n",
    "* `[batch_size, num_boxes, 5+num_classes]`\n",
    "\n",
    "Where the final dimension is `[x, y, w, h, objectness, score_class1, score_class2, ...]` and box coordinates are in pixels.\n",
    "\n",
    "Some newer versions (like the `ultralytics` versions we're using here) output a different format and omit the objectness score, so the wrapper will compute it as the highest-value class score for each batch index/box index combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8667c-6c48-4130-9473-7000bde4b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ultralytics.YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1116eb5e-8142-4b61-a585-a1038ea9173f",
   "metadata": {},
   "source": [
    "We don't need the class names for training, but it'll make our tensorboard logs more interpretable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ced763-5577-4359-8541-189286b328cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = em.YOLOWrapper(model.model.eval(), yolo_version=8, classnames=COCO_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c9597-2e26-428e-ab79-992da580d6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e4fc012-6156-41c2-9963-7c2f62b152d8",
   "metadata": {},
   "source": [
    "## assemble the pipeline\n",
    "\n",
    "Take all of the steps we built above and assemble into a `Pipeline` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca717a-ddc9-4876-ab46-da94d2cd7000",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = stacker+imp+aug+yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fda01b-036d-43b8-acd4-55896fcbedc4",
   "metadata": {},
   "source": [
    "## Write a loss function\n",
    "\n",
    "Loss and metrics need to be adapted fairly closely to the specific problem you're solving, so this part happens outside the `electricmayhem` API. Write a Python function that inputs the pipeline outputs (a list containing a rank-3 tensor) and has a `**kwargs` option (where any extra information generated by your pipeline stages will be available).\n",
    "\n",
    "The outputs should be a dictionary where each key is a metric or a term in your loss function, and each value is the **unaggregated** value (so each should be a 1D tensor of length `batch_size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14abe55a-c8d2-40c5-9a47-63d34b1c261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output, **kwargs):\n",
    "    # output[0] should be (batch, num_boxes, 5+num_classes)\n",
    "    # pull out max detection score across classes for every batch element and box\n",
    "    maxdetect_boxes = output[0][:,:,4] # (batch, num_boxes)\n",
    "    maxdetect = torch.max(maxdetect_boxes, 1)[0]  # (batch,)\n",
    "    # let's also compute an ASR at 25%\n",
    "    asr25 = (maxdetect < 0.25).type(torch.float32)\n",
    "\n",
    "    return {\"maxdetect\":maxdetect, \"asr25\":asr25}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893330cf-cce5-416c-9e08-f19838552e35",
   "metadata": {},
   "source": [
    "Pass the loss function to your pipeline along with a dictionary giving the shapes of a batch of test patches, so it can check the inputs/outputs before you start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827eb019-d791-475e-a6a5-e90b358a1e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.set_loss(loss, test_patch_shape={k:(2,1,64,64) for k in ['hood', 'roof', 'door']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99581ab7-3b8f-4d87-939b-2149b71cba4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc93e4b1-b9fe-482d-b765-2c640a7b47c5",
   "metadata": {},
   "source": [
    "## Train the patch\n",
    "\n",
    "We'll do this in four steps:\n",
    "\n",
    "1. Tell the pipeline where to log diagnostics (for TensorBoard) and metrics (in MLFlow)\n",
    "2. Initialize your patches\n",
    "3. Copy the pipeline to your GPU\n",
    "4. Train the actual patch!\n",
    "\n",
    "For the first step- start an MLFlow server if you need to by running `mlflow server` from the command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2333bac-fb9a-4990-8830-2fe7dfce739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.set_logging(logdir=\"logs/01_quickstart\",\n",
    "                    mlflow_uri=\"http://127.0.0.1:5000\",\n",
    "                    experiment_name=\"electricmayhem_tutorial_01_quickstart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463354a-84a4-4c0c-b002-39fb9e6e072c",
   "metadata": {},
   "source": [
    "Second, explicitly tell it to initialize the patches. If you want you could alternatively pass it a dictionary of patches pre-initialized to whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b40ed7-9198-4c40-ba66-444c95317ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.initialize_patch_params(patch_shape={k:(1,64,64) for k in ['hood', 'roof', 'door']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb53b19-4204-4aeb-b8b7-aaf7f5c06855",
   "metadata": {},
   "source": [
    "All of our classes inherit from `torch.nn.Module` so this should look familiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4ece5-c08f-48a1-9fdb-60bb88af9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68594a-3d7d-4cb8-b610-341691cb328e",
   "metadata": {},
   "source": [
    "Now, train the patch. The first two arguments of `pipeline.train_patch()` are the batch size and number of training steps.\n",
    "\n",
    "* `eval_every` and `num_eval_steps` sets how many steps between calling `pipeline.evaluate()` and how many eval batches to run each time.\n",
    "* `optimizer` can take values `'adam'`, `'bim'` (basic iterative method/iterative FGSM), or `'mifgsm'` (momentum-iterative FGSM)\n",
    "* Weights for every term in your loss function are set to zero unless specified here (so you have to specify at least one). That's the `maxdetect=1` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb106c-0369-47c4-8f33-c75d7a5aae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = pipeline.train_patch(\n",
    "    20,\n",
    "    1000,\n",
    "    learning_rate=0.01, \n",
    "    eval_every=10, #100,\n",
    "    num_eval_steps=10,\n",
    "    optimizer='adam',\n",
    "    lr_decay='cosine',\n",
    "    maxdetect=1.,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32052df1-6cc9-4525-bb87-4993d3b936ab",
   "metadata": {},
   "source": [
    "`pipeline.train_patch()` returns a dictionary containing the trained patches (though they'll all be saved as MLFlow artifacts too, just to be safe).\n",
    "\n",
    "The final evaluation results will be available in `pipeline.df` (also in MLFlow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f7607-bae4-46b5-a0b6-b558d34ea823",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f413df76-d045-4e8b-82b7-1dc58b8ae266",
   "metadata": {},
   "source": [
    "We can use these results to try and drill down on which factors in our pipeline impact patch performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ff7cd-7762-4b18-906e-dac64f4b826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "em.viz.eval_result_permutation_importance(pipeline.df, \"asr25_delta\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f05207-7b75-4232-8c56-bd732833f4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f159182-82ec-496d-96e5-8db992038ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd199de-4f1f-4bfa-bab5-ef6eba792c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
