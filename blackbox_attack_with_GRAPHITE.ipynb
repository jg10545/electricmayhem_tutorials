{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05cfb8a6-6e8e-4d9d-8b2c-7e1a069b7c71",
   "metadata": {},
   "source": [
    "# Blackbox patch optimization\n",
    "\n",
    "The `electricmayhem.blackbox` module is a lot less organized than its whitebox counterpart, and is currently effectively abandoned. For anyone interested in scavenging from what I've got here, though, this notebook will show an example of training a relatively crude patch against a COCO-trained YOLO model *without* access to gradients. \n",
    "\n",
    "## Background\n",
    "\n",
    "I strongly recomment taking a look at two things before trying this notebook:\n",
    "\n",
    "* Feng *et al*'s paper *GRAPHITE: Generating Automatic Physical Examples for Machine-Learning Attacks on Computer Vision Systems*. This paper outlines a relatively practical blackbox patch attack using zeroth-order gradient estimation; my code (mostly) follows Feng's method.\n",
    "* Docs for the Python library `dask`. The `BlackBoxPatchTrainer` class uses `dask` under the hood for parallelization; if you need to customize how it does scheduling, manages memory, or chooses the number of workers, you can use `dask`'s API to set global defaults.\n",
    "\n",
    "## Task for this notebook\n",
    "\n",
    "To keep it simple, we'll train a patch against a single image, using soft outputs from the model (scores, not just categories), with almost no augmentation. This means the patch will likely be overfit to this image and not generalize well to new backgrounds or orientations (but it will converge much more quickly).\n",
    "\n",
    "Using soft scores instead of hard decisions gives the RGF estimator more information, so the gradient estimate will be less noisy. Working directly with hard outputs would also require us to use some heuristic (like Feng's mask reduction technique) to find the decision boundary.\n",
    "\n",
    "We'll train a patch on an image from the toy car dataset, to try to cause the model to not detect the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15363fa3-9dc6-4f24-8507-8b6e6d5070a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import ultralytics\n",
    "import kornia.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e98d3-d22a-4f78-ad01-ad7f1db78646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import electricmayhem.whitebox as em\n",
    "from electricmayhem.blackbox import BlackBoxPatchTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158b41c-362d-4e7b-a4ac-24a5730026d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_CLASSES = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "                'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench','bird', 'cat',\n",
    "                'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
    "                'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "                 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "                'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "                 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
    "                'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "                'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book',\n",
    "                'clock', 'vase', 'scissors', 'teddy bear', 'hair drier','toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4499bfb1-c722-40d6-8ab1-fdc4f28a298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = em.load_to_tensor(\"data/toycar/medium_distance_arc/057.png\")\n",
    "em.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198f563-dd01-4b8b-8395-c30388c0d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import electricmayhem.blackbox\n",
    "electricmayhem.blackbox._augment.augment_image(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfabba8e-25bc-4977-a78f-de040e900ebd",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278f81d-e1a3-4cf4-8844-c18b030979f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ultralytics.YOLO(\"yolov8n.pt\").model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c69d4b0-7442-4edf-bdd5-2a71a3b26d88",
   "metadata": {},
   "source": [
    "## Write a detection function\n",
    "\n",
    "This can be pretty much any dask-compatible function that interfaces with the model we're attacking. In this case it will wrap a Pytorch model; it could just as easily be calling out to an external API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97184d-46ae-4370-b869-3861725e4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_func(img, return_raw=False, **kwargs):\n",
    "    img = kornia.geometry.transform.resize(img, (640,640))\n",
    "    model_outputs = model(img.unsqueeze(0))[0]\n",
    "    class_probs = model_outputs[:,4:,:]\n",
    "    maxval = torch.max(class_probs).item()\n",
    "    if return_raw:\n",
    "        return [int(maxval>0.25),maxval]\n",
    "    else:\n",
    "        return maxval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77781e03-fbd3-47f3-b94c-db833e5c1b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b03d55-e31f-479d-9130-c6299452caed",
   "metadata": {},
   "source": [
    "## Create a mask\n",
    "\n",
    "For hard blackbox attacks that need to find a decision boundary, we'd need two masks- an \"initial\" one (that covers up the object) and a \"final\" one (giving the shape of the final patch we want), and the algorithm would try to coevolve the mask and patch together- gradually reducing the initial mask to get to the final one, without straying too far from the decision boundary.\n",
    "\n",
    "For the soft blackbox attack we can just use the final mask, since we can differentiate between a detection at 90% confidence and one at 89% confidence (which look the same in the hard case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762a766-2b87-40a2-b203-5e0d16d4e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros_like(x)\n",
    "mask[:,300:600,50:450] += 1\n",
    "em.plot(x*(1-mask) + mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09734919-b7c1-4a4d-86d0-c52d62641845",
   "metadata": {},
   "source": [
    "## Add a TensorBoard callback\n",
    "\n",
    "The \"transform robustness\" metric in Feng's paper isn't a perfect measure for this example, so let's add a callback function that will record a plot of YOLO detections to TensorBoard every eval step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372c9f8-d61d-4f82-9d4f-1d920a2c6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_EVERY = 190\n",
    "def eval_func(writer, counter, img, mask, perturbation, **kwargs):\n",
    "    if counter % SAVE_EVERY == 0:\n",
    "        pert = kornia.geometry.transform.resize(perturbation, (640,640))\n",
    "        x = img*(1-mask) + mask*pert\n",
    "        detections = model(x.unsqueeze(0))\n",
    "        detections_converted = em._yolo.convert_ultralytics_to_v5_format(detections[0])[0]\n",
    "        fig = em._yolo.plot_detections(x.unsqueeze(0), detections_converted, 0, classnames=COCO_CLASSES)\n",
    "        writer.add_figure(\"detections\", fig, global_step=counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d51db3-236e-4cdb-a901-c632b2c506d9",
   "metadata": {},
   "source": [
    "## Set up a trainer\n",
    "\n",
    "The required arguments are:\n",
    "\n",
    "* The target image as a Pytorch tensor\n",
    "* The initial mask, as an image tensor\n",
    "* The final mask, as an image tensor. For this example initial and final masks are the same.\n",
    "* The detection function\n",
    "* A directory to save logs in\n",
    "\n",
    "The keyword arguments after that set some options:\n",
    "\n",
    "* `perturbation` specifies the initial patch. It can be any size, but will be resized to the target image dimensions before applying the mask. Lowering the resolution of this patch lowers its flexibility to defeat the model but also lowers the variance of the RGF estimator, effectively letting you tune a bias-variance tradeoff for the blackbox attack.\n",
    "* `num_augments` sets the number of augmentations it will test every random direction under for every step of RGF. Since augmentation is effectively disabled here we'll just set it to 1. When set higher, `dask` will attempt to parallelize the computation.\n",
    "* Following Feng *et al*, `q` is the number of random directions we'll query for each step of RGF.\n",
    "* `beta` is the RGF smoothing parameter. The estimator becomes unbiased in the limit $\\beta \\rightarrow 0$.\n",
    "* `reduce_mask` defaults to `True`. `False` skips Feng *et al*'s mask reduction step.\n",
    "* `eval_augments` sets the number of augmentations to test under for evaluation.\n",
    "* `eval_func` takes an optional function to add custom logs to tensorboard\n",
    "* `extra_params` for anything else you want recorded to MLFlow\n",
    "* `aug_params` configures augmentations. In addition to most of the ones in Feng's paper, I have a couple options for composition noise that shifts or rotates the perturbation with respect to the target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ccdfdb-007e-4b30-8d7f-cd198c5e4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 120 \n",
    "perturbation = torch.tensor(np.random.uniform(0, 1, size=(3,d,d)).astype(np.float32))\n",
    "\n",
    "trainer = BlackBoxPatchTrainer(\n",
    "    x, # the target image\n",
    "    mask, # initial mask- in this case, same as the final mask\n",
    "    mask, # final mask\n",
    "    detect_func,\n",
    "    \"logs/blackbox_example\",\n",
    "    perturbation=perturbation,\n",
    "    num_augments=1, # number of augmentations to test each random direction under\n",
    "    q=10, # number of random directions per step\n",
    "    beta=0.1, # RGF smoothing parameter\n",
    "    reduce_mask=False, # skipping the mask-reduction step\n",
    "    eval_augments=50, # number of random augmentations to use for evaluation\n",
    "    use_scores=True, # use soft outputs of the model rather than hard decisions\n",
    "    eval_func=eval_func,\n",
    "    extra_params={\"perturbation_size\":d}, # additional parameters we might want to log to MLFlow\n",
    "    aug_params={\"scale\":(0.99,1.01), \"blur\":[0], \"rotate\":0, \"angle\":0, \"translate\":0, \"gamma\":(1,1.1),\n",
    "               \"perspective_scale\":0},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80b008-7cb4-4c26-972a-b9a14bbb89bd",
   "metadata": {},
   "source": [
    "To fit the patch- you can train for a fixed number of epochs or a fixed query budget. The `lrs` kwarg lets you specify the RGF step sizes that will be tested for each update. The chosen learning rate will get recorded in tensorboard; if it's pinned at the min or max value you may need to expand the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5ecc4-2313-4608-80d1-3ea9e1f281ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(budget=1000000, lrs=[1e5, 1e4, 1000., 100., 10., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e324324-82bc-4e35-b5a7-8d3dbe3ee423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b19be9-9427-49e1-acb0-5971e12e7719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e33e35-92cc-4670-806a-3100d83695dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d31bf4-734e-4845-b09e-34c80ca7e3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
